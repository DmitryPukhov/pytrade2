{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f864f-c6ae-430f-840a-28ab145c4d8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#%conda install -c plotly plotly=5.9.0\n",
    "#%conda install pip\n",
    "#%conda install twisted\n",
    "\n",
    "%pip install plotly==5.9.0\n",
    "%pip install twisted\n",
    "%pip install pandas\n",
    "%pip install ta    \n",
    "%pip install scikit-learn\n",
    "%pip install tensorflow\n",
    "%pip install keras\n",
    "%pip install matplotlib\n",
    "%pip install scikeras\n",
    "%pip install keras-tuner\n",
    "%pip install plotly\n",
    "%pip install nbformat\n",
    "#%pip install huobi-sdk==2.3.3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np    \n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)\n",
    "\n",
    "import os\n",
    "# Downloading latest pytrade2 data\n",
    "#os.system(\"cd ./../deploy/yandex_cloud; ./download_data.sh\")\n",
    "data_dir=f\"../data/dev/common\"\n",
    "#print(f\"Download completed. Local data dir: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c1bd4-5b11-4d34-8543-3b795022e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol=\"BTC-USDT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13faa34f-cdae-4784-b646-702cb6c3ac91",
   "metadata": {},
   "source": [
    "## Read candles history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52109b70-d9d0-4a28-a214-829f1bf3d53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def read_candles(days = 1, period = '1min'):\n",
    "    \"\"\" Read last days' 1 min candles from file system \"\"\"\n",
    "\n",
    "    candles_dir = Path(data_dir, 'candles')\n",
    "    files = sorted([f for f in os.listdir(candles_dir) if f.endswith(f\"candles_{period}.csv\")])\n",
    "    # Read last days' files to one dataframe\n",
    "    df = pd.concat([pd.read_csv(Path(candles_dir, fname), parse_dates=['open_time', 'close_time']) for fname in files[-days:]])\n",
    "    df = df.set_index('close_time', drop=False)\n",
    "    #del(df['close_time.1']) # temp fix\n",
    "    # Resample because row data contains multiple candles inside a period\n",
    "    print(f'Read {len(df)} candles from {df.index.min()} to {df.index.max()}')\n",
    "    return df.resample('1min').agg('last')\n",
    "\n",
    "candles = read_candles(days=10)\n",
    "candles.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d354c44d-62dd-49f1-889e-43dd6f52d8b8",
   "metadata": {},
   "source": [
    "## Calculate targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7e85d7-3ff2-4e60-92cd-cd26a8bb7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_value_counts(ax, df, col, grouped):\n",
    "    signals = df[col]\n",
    "    #vc = signals[signals.diff() != 0].value_counts()\n",
    "    vc = df[col].value_counts() if not grouped else signals[(signals.diff() != 0) & (signals != 0)].value_counts()\n",
    "    label_map={0:'oom', 1:'buy', -1: 'sell'}\n",
    "    color_map={'oom':'C0', 'buy': 'C1', 'sell': 'C2'}\n",
    "    labels = [ label_map[signal] for signal in vc.index.tolist()]\n",
    "    colors = [color_map[key] for key in labels]\n",
    "    ax.pie(vc, labels = labels,  autopct= lambda x: '{:.0f}'.format(x*vc.sum()/100), colors = colors)\n",
    "    tag = 'groups' if grouped else ''\n",
    "    ax.set_title(f\"{col} {tag}\")\n",
    "    \n",
    "def plot_signal_counts(df):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2) \n",
    "    plot_value_counts(ax1, df, 'signal', grouped = False)\n",
    "    plot_value_counts(ax2, df, 'signal', grouped = True)\n",
    "    fig.suptitle(f'Signal counts from {df.index.min()} to {df.index.max()}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca19f0e-0bc5-4f0b-8d4c-ed55098f567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_signal(candles, predict_window, open_ratio, min_profit_ratio, max_loss_ratio, comission_pct=0.012):\n",
    "    \"\"\" Signal -1,0,1 plus extended debugging info \"\"\"\n",
    "    #next_candles = candles[['open', 'high', 'low', 'close', 'vol']].shift(1)\n",
    "\n",
    "    next_candles = pd.DataFrame(index=candles.index)\n",
    "    next_candles[['high', 'low']] = (candles[['high', 'low']][::-1]\n",
    "                                     .rolling(predict_window).agg({'high' : 'max', 'low' : 'min'})\n",
    "                                     [::-1])\n",
    "\n",
    "    # BTC-USDT 40 000 * 1% = 400\n",
    "    # BTC-USDT 40 000 * 0.012% = 40 * 0.012 = 4,8\n",
    "    comission = comission_pct*0.01*2 # Order open, order close, double comission\n",
    "    # Ratio to open: generate signal if profit/loss > open ratio\n",
    "    #open_ratio = 1\n",
    "    min_profit = candles['close']*(max(comission*2, min_profit_ratio))\n",
    "    max_loss = candles['close']*max_loss_ratio\n",
    "    \n",
    "    # Profit / loss > open ratio considering comission and minimal profit\n",
    "    profit_buy = (next_candles['high'] - candles['high']) - (candles['close']*2*comission)\n",
    "    loss_buy =  (candles['high'] - next_candles['low']) + (candles['close']*2*comission)\n",
    "    signal_buy = (profit_buy > 0) & ((profit_buy / loss_buy) > open_ratio)& (profit_buy > min_profit) & (loss_buy < max_loss)\n",
    "    \n",
    "    # Profit / loss > open ratio considering comission and minimal profit\n",
    "    profit_sell = (candles['low'] - next_candles['low']) - (candles['close']*2*comission)\n",
    "    loss_sell = (next_candles['high'] - candles['low']) + (candles['close']*2*comission)\n",
    "    signal_sell = (profit_sell > 0) & ((profit_sell / loss_sell) > open_ratio) & (profit_sell > min_profit) & (loss_sell < max_loss)\n",
    "\n",
    "    # Signal\n",
    "    signal = pd.DataFrame(index=candles.index)\n",
    "    signal['signal'] = 0  # Default to 0\n",
    "    signal.loc[signal_buy & ~signal_sell, 'signal'] = 1  # Set to 1 where 'buy' is True and 'sell' is False\n",
    "    signal.loc[~signal_buy & signal_sell, 'signal'] = -1  # Set to -1 where 'sell' is True and 'buy' is False\n",
    "\n",
    "    df = signal\n",
    "    # Future profit and loss\n",
    "    df.loc[signal['signal']==1, 'profit'] = profit_buy\n",
    "    df.loc[signal['signal']==1, 'loss'] = loss_buy\n",
    "    df.loc[signal['signal']==-1, 'profit'] = profit_sell\n",
    "    df.loc[signal['signal']==-1, 'loss'] = loss_sell\n",
    "    return df\n",
    "\n",
    "signal = (calc_signal(candles, \n",
    "                      predict_window = '15min', \n",
    "                      open_ratio = 2, \n",
    "                      # 0.01*0.012*2 - open+close comission 0.012%\n",
    "                      # Comission is 4.8 for 40K price, ~10 for one open+close trade\n",
    "                      min_profit_ratio = (0.01*0.012*2) * 5,\n",
    "                      max_loss_ratio = (0.01*0.012*2) * 5)\n",
    "         )\n",
    "signal[signal['signal']!=0].tail(100)\n",
    "\n",
    "# Pie plot, signals\n",
    "plot_signal_counts(signal)\n",
    "\n",
    "\n",
    "# Line plot, profit/loss\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "# Supress pandas+plotly warning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "px.line(signal, title='profit/loss').update_traces(mode='lines+markers').show()\n",
    "\n",
    "# Matplotlib profit/loss chart\n",
    "# signal[signal['signal']!=0].plot(linestyle=':', marker='o', title='profit/loss')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2509499e-8e6b-4073-9a2e-49e96e3cd23c",
   "metadata": {},
   "source": [
    "\n",
    "## Feature engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f0096-0d8b-436d-9c84-4674e93b636f",
   "metadata": {},
   "source": [
    "### Calculate indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a403ca4-3799-45a6-a60f-6df795f95c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ta import trend, momentum, volume, others, volatility\n",
    "\n",
    "\n",
    "\n",
    "def with_time_features(df: pd.DataFrame):\n",
    "    # dt = df.index.to_frame()[\"close_time\"].dt\n",
    "    dt = df.index\n",
    "    df[\"time_hour\"] = dt.hour\n",
    "    df[\"time_minute\"] = dt.minute\n",
    "    #df[\"time_second\"] = dt.second\n",
    "    return df\n",
    "\n",
    "def with_ichimoku(candles: pd.DataFrame):\n",
    "    ichimoku = trend.IchimokuIndicator(candles['high'], candles['low'])\n",
    "    candles['ichimoku_base_line'] = ichimoku.ichimoku_base_line()\n",
    "    candles['ichimoku_conversion_line'] = ichimoku.ichimoku_conversion_line()\n",
    "    candles['ichimoku_a'] = ichimoku.ichimoku_a()\n",
    "    candles['ichimoku_b'] = ichimoku.ichimoku_b()    \n",
    "    return candles\n",
    "\n",
    "# Apply features and indicators\n",
    "candles = with_time_features(candles)\n",
    "candles = with_ichimoku(candles)\n",
    "candles['cci'] = trend.cci(candles['high'], candles['low'], candles['close'])\n",
    "candles['adx'] = trend.adx(candles['high'], candles['low'], candles['close'])\n",
    "candles['rsi'] = momentum.rsi(candles['close'])\n",
    "candles['stoch'] = momentum.stoch(candles['high'], candles['low'], candles['close'])\n",
    "candles['macd'] = trend.macd(candles['close'])\n",
    "\n",
    "\n",
    "candles.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb7a63-d87b-46b7-aef6-4d3eebc5329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly as py\n",
    "from plotly import graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_indicators(df, extra_lines:[], extra_subplots:[]):\n",
    "    fig = make_subplots(rows=len(extra_subplots)+1, cols=1, \n",
    "                        shared_xaxes=True,\n",
    "                        vertical_spacing=0.02,\n",
    "                        subplot_titles=['candles']+extra_subplots\n",
    "                       )    \n",
    "    \n",
    "    # Candles\n",
    "    fig.add_trace(go.Candlestick(\\\n",
    "                        name='candles',\n",
    "                        x=df.index,\\\n",
    "                        open=df['open'],\\\n",
    "                        high=df['high'],\\\n",
    "                        low=df['low'],\\\n",
    "                        close=df['close'])\n",
    "                     , row=1, col=1)\n",
    "\n",
    "    # Other charts on candles chart\n",
    "    for name in extra_lines:\n",
    "        fig.add_trace(go.Scatter(name=name, x=df.index, y=df[name], mode='lines'), row=1, col=1)\n",
    "\n",
    "    # Other subplots below candles\n",
    "    for i, col in enumerate(extra_subplots):\n",
    "        fig.add_trace(go.Scatter(name=col, x=df.index,y=df[col], mode='lines'), row=2+i, col=1)\n",
    "    \n",
    "    fig.update_layout(title=f\"{candles['ticker'][0]}\",\n",
    "                    xaxis_rangeslider_visible=False, \n",
    "                    height=300*len(extra_subplots))\n",
    "    fig.show()\n",
    "\n",
    "ichimoku_cols = ['ichimoku_base_line', 'ichimoku_conversion_line', 'ichimoku_a', 'ichimoku_b']\n",
    "indicators_cols = ['cci', 'adx', 'rsi', 'stoch']\n",
    "plot_indicators(candles, \n",
    "             extra_lines=ichimoku_cols,\n",
    "             extra_subplots=indicators_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350f9725-e77d-4472-a963-2b0c5d989507",
   "metadata": {},
   "source": [
    "### Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5298f98-1af4-4a91-95e6-6aba787f13c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced(features, targets):\n",
    "    \"\"\" Balance by signal \"\"\"\n",
    "    cnt = min(targets.value_counts())\n",
    "    balanced_targets = pd.concat([targets[targets['signal'] == signal].sample(cnt) for signal in [-1,0,1]]).sort_index()\n",
    "    balanced_features = features[features.index.isin(balanced_targets.index)].sort_index()\n",
    "    return balanced_features, balanced_targets\n",
    "    \n",
    "    \n",
    "def get_features_targets(candles, signal):\n",
    "\n",
    "    feature_cols = ['time_hour', 'time_minute'] + ichimoku_cols + indicators_cols\n",
    "    features = candles[feature_cols]\n",
    "    targets = signal.loc[features.index,['signal']]\n",
    "    \n",
    "    return features, targets\n",
    "\n",
    "features, targets = get_features_targets(candles, signal)\n",
    "features, targets = balanced(features, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62e21a-1756-44a7-96fa-06ebe089ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daf566d-272a-4d72-851e-faa84485192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "class DiffTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x):\n",
    "        x_diff = x.copy()\n",
    "        for col in self.columns:\n",
    "            #x_diff[col + '_diff'] = x_diff[col].diff()\n",
    "            x_diff.loc[:,col] = x_diff[col].diff()\n",
    "        return x_diff\n",
    "        #return x_diff.drop(columns=self.columns)\n",
    "\n",
    "def create_pipe(X, y) -> (Pipeline, Pipeline):\n",
    "    \"\"\" Create feature and target pipelines to use for transform and inverse transform \"\"\"\n",
    "\n",
    "    time_cols = [col for col in X.columns if col.startswith(\"time\") or col.endswith(\"time\")]\n",
    "    float_cols = list(set(X.columns) - set(time_cols))\n",
    "\n",
    "    # Scale x\n",
    "    x_pipe = Pipeline(\n",
    "        [\n",
    "        #(\"diff_transform\", DiffTransformer(columns=float_cols)),\n",
    "        (\"xscaler\", ColumnTransformer([(\"xrs\", RobustScaler(), float_cols)], remainder=\"passthrough\")),\n",
    "        (\"xmms\", MinMaxScaler())])\n",
    "    x_pipe.fit(X)\n",
    "\n",
    "    # One hot encode y\n",
    "    y_pipe = Pipeline([('adjust_labels', OneHotEncoder(categories=[[-1, 0, 1]], sparse_output=False, drop=None))])\n",
    "    y_pipe.fit(y)\n",
    "    return x_pipe, y_pipe\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, targets, shuffle=False)\n",
    "\n",
    "x_pipe, y_pipe = create_pipe(x_train, y_train)\n",
    "x_train = x_pipe.transform(x_train)\n",
    "x_test = x_pipe.transform(x_test)\n",
    "y_train = y_pipe.transform(y_train)\n",
    "y_test = y_pipe.transform(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61650f6-0be2-4a18-8a4a-f18eeb034482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential, Input\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import collections\n",
    "\n",
    "# def create_model_prod(X_train, y_train, window_size):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(128,  return_sequences=True, input_shape=(window_size, X_train.shape[1])))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(LSTM(32))         \n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(20, activation='relu'))\n",
    "#     model.add(Dense(y_train.shape[1], activation='linear'))\n",
    "#     #model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "#     model.compile(optimizer='adam', loss='mae', metrics=['mse'])\n",
    "#     return model  \n",
    "plot_figsize=(10,5)\n",
    "#def create_model(X_train, y_train, window_size, lstm1_units, lstm2_units, dense1_units, dense2_units):\n",
    "def create_models(*specs):\n",
    "    results={}\n",
    "    for unit_spec in specs:\n",
    "        window_size=unit_spec[0]\n",
    "        yield create_model(X_train, y_train, window_size, unit_spec[1:])\n",
    "\n",
    "def create_model(x_train,  y_train, specs):\n",
    "    \"\"\" Create model with layers given in specs \"\"\"\n",
    "\n",
    "    input_shape=(x_train.shape[1],)\n",
    "    print(f\"Creating model({specs}), input shape={input_shape}\")\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    # Keras layers\n",
    "    for units in specs:\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax')) # Softmax for classification\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_history_plotly(df, title):\n",
    "    px.line(df,title=title).update_traces(mode='lines+markers').show()\n",
    "    \n",
    "def plot_history_plt(df, title):\n",
    "#         for name in names:\n",
    "#             plt.plot(history.history[name])\n",
    "        plt.plot(df)\n",
    "        #Captions and show the plot\n",
    "        plt.title(title)\n",
    "        #plt.ylabel(metric_name)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(df.columns)\n",
    "        #plt.legend(names, loc='upper left')\n",
    "        plt.show()    \n",
    "\n",
    "def plot_history(model_name, history, plot_func, metric=None):\n",
    "    \"\"\" Plot history loss and metrics\"\"\"\n",
    "    metric_names = [metric] if metric else history.history\n",
    "    # Print all merrics\n",
    "    for metric_name in metric_names:\n",
    "        # Validation metrics names will be calculated from related train metrics\n",
    "        if metric_name.startswith(\"val_\"): continue\n",
    "\n",
    "        # Plot metric and related test (val_..) metric\n",
    "        plt.figure(figsize=plot_figsize)\n",
    "        title=f\"{model_name} {metric_name}\"\n",
    "        names=[metric_name, f\"val_{metric_name}\"]\n",
    "        \n",
    "        df=pd.DataFrame()\n",
    "        for name in names:\n",
    "            df[name] = history.history[name]\n",
    "        plot_func(df, title)\n",
    "        #px.line(df,title=title).update_traces(mode='lines+markers').show()\n",
    "\n",
    "        \n",
    "#         for name in names:\n",
    "#             plt.plot(history.history[name])\n",
    "        # Captions and show the plot\n",
    "#         plt.title(f\"{model_name} {metric_name}\")\n",
    "#         plt.ylabel(metric_name)\n",
    "#         plt.xlabel('epoch')\n",
    "#         plt.legend(names, loc='upper left')\n",
    "#         plt.show()\n",
    "\n",
    "def fit_model(model, train_gen, test_gen, epochs):\n",
    "    # Fit the model\n",
    "    steps_per_epoch=5\n",
    "    history=model.fit(train_gen, validation_data=test_gen, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_models(epochs=10, *specs):\n",
    "    results={}\n",
    "    for unit_spec in specs:\n",
    "        model = create_model(x_train, y_train, unit_spec)\n",
    "        \n",
    "        steps_per_epoch=15\n",
    "        history=model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
    "        model_name = f\"Model({unit_spec})\"\n",
    "        results[model_name] = history\n",
    "    return results\n",
    "\n",
    "def plot_res(results, plot_func):\n",
    "    for model_name in results:\n",
    "        plot_history(model_name, results[model_name], plot_func, None)\n",
    "\n",
    "\n",
    "        \n",
    "# Good: Current LSTM2: 10, 320, 0.2, 160, 0.2, 40, 0.2, 16, 0.1\n",
    "\n",
    "eval_res = evaluate_models(20, \n",
    "                                  [32, 128, 64, 16]\n",
    "                                )\n",
    "\n",
    "plot_res(eval_res, plot_history_plt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11env",
   "language": "python",
   "name": "python3.11env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
